---
title: "Class 7: Machine Learning 1"
author: "Ashley Martinez (PID: A17891957)"
format: pdf
---

Today we will explore some fundamental machine learning methods including clustering and dimensionality reduction.

## K-means clustering

To see how this works let's first makeup some data to cluster where we know what the answer should be. We can use the 'rnorm()' function to help here:


```{r}
hist(rnorm(500,mean=5))
```

```{r}
x<-c(rnorm(30,mean=-3), rnorm(30,mean=3))
y<-rev(x)
```


```{r}
x<-cbind(x,y)
plot(x)
```


The function for K-means clustering in "base" R is 'kmeans()'

```{r}
k<-kmeans(x,centers=2)
k
```


To get the results of the returned list object we can dollar '$' syntax

>Q. How many points are in each cluster?

```{r}
k$size
```

>Q. What 'component' of your result object details
  -cluster assignment/membership?
  -cluster center?
  
```{r}
k$cluster
```

```{r}
k$centers
```



>Q. Make a clustering results figure of the data colored by cluster membership.

```{r}
plot(x,col=c("red","blue"))
```

```{r}
plot(x,col=k$cluster,pch=16)
```


```{r}
plot(x,col=k$cluster,pch=16)
points(k$centers,col="blue",pch=15,cex=2)
```


K-means clustering is very popular as its very fast and relatively straight foward: it takes numeric data as input and returns the clusterm membership vector etc.

The "issue" is we tell 'kmeans()' how many clusters we want!

>Q. Run kmeans again and cluster into 4 grps/clusters and plot the results like we did above?

```{r}
k4<-kmeans(x,centers=4)
plot(x,col=k$cluster)
points(k4$centers,pch=15)
```

Screeplot to pick k 'centers' calue

Can do through brute force

```{r}
c(k1<-kmeans(x,centers=1),
  k2<-kmeans(x,centers=2),
  k3<-kmeans(x,centers=3),
  k4<-kmeans(x,centers=4),
  k5<-kmeans(x,centers=5))
```

```{r}
z<-c(k1$tot.withinss,
  k2$tot.withinss,
  k3$tot.withinss,
  k4$tot.withinss,
  k5$tot.withinss)
plot(z,)
```


```{r}
n<-NULL
for(i in 1:5) {
  n<-c(n,kmeans(x,centers=i)$tot.withinss)
}
plot(n,typ="b")
```

##Hierarchial Clustering

The main "base" R function for Hierarchal Clustering is called 'hclust()'. Here we can't just input our data we need to first calculate a distance matrix (e.g. 'dist()') for our data and use this as input to 'hclust()'

```{r}
d<-dist(x)
hc<-hclust(d)
hc
```

There is a plot method for hclust result lets try it, notice how the higher numbers are on the right side.

```{r}
plot(hc)
abline(h=8,col="red")
cutree(hc,h=8)
```

To get our cluster "membership" vector (i.e. our main clustering result) we can "cut" the tree at a given height or at a height that yields a given "k" groups.

```{r}
cutree(hc,h=8)
```

```{r}
grps<-cutree(hc,k=2)
```

>Q. Plot the data with out hclust result coloring

```{r}
plot(x,col=grps)
```

#Principal Component Analysis (PCA)

##PCA of UK Food Data

Import food data from an online CSV file:"https://tiny.url.com/UK-foods"

```{r}
url<-("https://bioboot.github.io/bggn213_f17/class-material/UK_foods.csv")
x <- read.csv(url)
head(x)
```


```{r}
rownames(x)<-x[,1]
x<-x[,-1]
x
```

```{r}
x<-read.csv(url,row.names=1)
x
```

Some base figures

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

There is one plot that can be useful for small datasets:

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

>Main point: It can be difficult to spot major trends and patterns even in relatively small multivariate datasets (here we only have 17 dimensions, typically we have 1000s).

##PCA will help

The main function in "base" R for PCA is called 'prcomp()'
I will take transpose of our data so the "food" are in the columns:

```{r}
pca<-prcomp(t(x))
summary(pca)
```

```{r}
pca$x
cols<-c("orange", "red","blue","darkgreen" )
plot(pca$x[,1],pca$x[,2],col=cols, pch=16)
```

```{r}
library(ggplot2)
```

```{r}
ggplot(pca$x)+
  aes(PC1,PC2)+
  geom_point(col=cols)
```

```{r}
ggplot(pca$rotation)+
  aes(PC1, rownames(pca$rotation))+
  geom_col()
```

PCA looks super useful and we will come back to describe this further the next day.

