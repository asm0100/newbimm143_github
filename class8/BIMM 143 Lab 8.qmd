---
title: "Class8"
author: "Ashley Martinez (PID: A17891957)"
format: pdf
toc: true
---

## Background
The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

## Data Import
Here we downloaded the file to our laptops then added it to our BIMM 143 folder. Then it will appear in the "Files" on the right and can be added into wisc.df.

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
```

Now let's view some of our data.

```{r}
head(wisc.df)
```

Let's remove the first column from the data.

```{r}
wisc.data <- wisc.df[,-1]
```

Let's create a vector for later that contains the diagnosis column data.

```{r}
diagnosis<-factor(wisc.df$diagnosis)
```

>Q1. How many observations are in the diagnosis dataset?

```{r}
length(wisc.df$diagnosis)
```

>Q2. How many observations are malignant?

```{r}
table(wisc.df$diagnosis)
```

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
allcolnames<-colnames(wisc.data)
grep("_mean",allcolnames,value=TRUE)
```

## Principal Component Analysis (PCA)

We need to determine if we should perform a PCA on the data. If units are different across the data it will likely be beneficial to scale the data.

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

As we can see it is worth performing a PCA.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

>Q4. What proportion of the orginial variance is captured by the first principal components?

Answer: 0.4427

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Answer: For this we look at cumulative proportion. PC3 contains 0.72636 of the original variance.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Answer: PC7.

```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Answer: What stands out is how much information is shown at once making it hard for me to understand. There are numbers on all four sides of the graph and too much overlap at data at once.

```{r}
plot( wisc.pr$x[, 1:2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```


```{r}
plot(wisc.pr$x[, 1:2 ], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

Answer: They are seperating the spread between malignant and benign tumors.

The main PCA result figure is called a "score plot" or "PC plot" or "ordination plot"...

```{r}
library(ggplot2)
```

 
Lets plot PC1 vs PC2.

```{r}
ggplot(wisc.pr$x)+ 
  aes(PC1,PC2,col=diagnosis)+
  geom_point()
```


Let's calculate the variance of each component in wisc.pr.

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

## Communicating PCA results

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

Answer: -0.2609

```{r}
summary(wisc.pr$rotation["concave.points_mean","PC1"])
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

Answer: PC5

## Hierarchical clustering

Just clustering the original data is not very informative or helpful.

```{r}
data.scaled<-scale(wisc.data)
data.dist<-dist(data.scaled)
wisc.hclust<-hclust(data.dist)
plot(wisc.hclust)
```


>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

Answer: h=19

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```


```{r}
wisc.hclust.clusters<-cutree(wisc.hclust,k=2)
table(cutree(wisc.hclust,k=2))
```

```{r}
table(wisc.hclust.clusters,diagnosis)
```

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Answer: Two clusters provides a more even distribution between benign and malignant.

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Answer: I prefer ward.D2 the most because the plots themselves are very busy with data and this method most clearly conveys the high amount of information.

```{r}
wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)
```

```{r}
table(wisc.km$cluster, diagnosis)
```


>Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

Answer: HClust once adjusted to two clusters does a slightly better job in my opinion of seperating the results into a more even distribution while k-means does a decent job but not as well. HClust seperated benign and malignant into roughly 350 and 200 while the distribution for k-means is about 350 to 130.

## Combining methods (PCA and Clustering)

Clustering the original data was not productive. The PCA results looked promising. Here we combine these methods by clustering from our PCA results. In other words "clustering in PC space".

```{r}
## Take the first 3 PCs
dist.pc<-dist(wisc.pr$x[,1:3])
wisc.pr.hclust<-hclust(dist.pc,method="ward.D2")
```

```{r}
plot(wisc.pr.hclust)
abline(h=70,col="red")
```

To get our clustering membership vector (i.e. our main clustering result) we "cut" the tree at a desired height or to yield a desired number of "k" groups.

This is the code being used but for the sake of not adding 5 pages it will not be executed.

cutree(wisc.pr.hclust,k=2)



```{r}
grps<-cutree(wisc.pr.hclust,h=70)
table(grps)
```

How does this clustering grps compare to the expert?

```{r}
table(grps,diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
wisc.pr.dist <- dist(wisc.pr$x[, 1:7])
wisc.pr.hclust <- hclust(wisc.pr.dist, method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters,diagnosis)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

Answer: The new model doesn't do the best job of separating the two diagnoses with four clusters as they are very skewed.

>Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

Answer: In this case the k-means method provides a better seperation of the data because the data is more evenly distributed between the clusters.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```
##Sensitivity 

Sensitivity: TP/(TP+FN)
Specificity: TN/(TN+FN)

K-means:
Sensitivity: 175/(175+37)=0.825
Specificity: 343/(343+357)=0.961

Hierarchical:
Sensitivity: 172/(172+40)=0.811
Specificity: 343/(343+14)=0.961

>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Answer: The K-means has slightly higher sensitivity but both have equal specificity.

## 7. Prediction

We can use our PCA model for prediction with new input patient samples.

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18. Which of these new patients should we prioritize for follow up based on your results?

Answer: Patient 2 should be followed up with because malignent tumors have been seen on the lower end of PC1.

```{r}
sessionInfo()
```



